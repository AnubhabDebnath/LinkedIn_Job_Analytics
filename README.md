# 
# <img src="https://media.tenor.com/KOki-OrS24AAAAAC/linkedin.gif" width="48" height="48"> **Introduction**

This depository contains the analysis of all the jobs available at linkedin website, it goes through various stages and during the process has encountered a few problems. By tackling those problems I have successfully got a few insights which will help potential job seekers and recruiters to get their best fit. The data scraping part was done in python, cleaning was done using excel and the queries needed were run using mysql again the final dashboard was made on excel.



![image](https://media.tenor.com/gyNDu8UeHA8AAAAd/looking-for-a-job-job.gif)



# <img src="https://media.tenor.com/lvLaG5hPCncAAAAd/data-analysis.gif" width="48" height="48"> **Data Description**

# <img src="https://media.tenor.com/9GwjnjhCSZ0AAAAi/sportsmanias-technical-difficulties.gif" width="48" height="48"> *Problems Faced*

The first problem we encountered was with the new libraries that we needed to learn about to pull in data from LinkedIn. Selenium and BeautifulSoup were two libraries we did not know about before so, we had to learn it from scratch and research a lot online in order to use it in our project. Finally we need manage to scrape the data but the next problem waiting for us was the cleaning of the data. The raw data was really hard to understand and I had to work on it for hours to make it usable and clean.
  1. The first problem was fixed using BeautifulSoup and Selenium WebDriver, we managed to pull the data from LinkedIn using BeautifulSoup and automated the process using Selenium for things like changing the page number etc.
  2. The second challenging problem was solved using excel, I had to use various tools as well as functions to clean the dataset thoroughly as it was a very essential part of our project, specially when the next step was to run queries in sql.





